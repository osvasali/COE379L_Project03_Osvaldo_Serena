{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f675411",
   "metadata": {},
   "source": [
    "# Project 2 Model Classification\n",
    "### Serena Shah, Osvaldo Salinas\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966bfb7",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87799c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"data/train/damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/train/no_damage\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(\"data/test/damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/test/no_damage\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93b108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need paths of images for individual classes so we can copy them in the new directories that we created above\n",
    "\n",
    "damage_all_paths = os.listdir('data_all_modified/damage')\n",
    "no_damage_all_paths = os.listdir('data_all_modified/no_damage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea54e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      "train damage image count:  11336\n",
      "test damage image count:  2834\n",
      "len of overlap:  0\n",
      "...............\n",
      "\n",
      "...............\n",
      "train no damage image count:  5721\n",
      "test no damage image count:  1431\n",
      "len of overlap:  0\n",
      "...............\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the image paths into train and test by randomly selecting 80% of the images in train and 20% in test.\n",
    "import random\n",
    "\n",
    "print(\"...............\")\n",
    "train_damage_paths = random.sample(damage_all_paths, int(len(damage_all_paths)*0.8))\n",
    "print(\"train damage image count: \", len(train_damage_paths))\n",
    "test_damage_paths = [ p for p in damage_all_paths if p not in train_damage_paths]\n",
    "print(\"test damage image count: \", len(test_damage_paths))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in train_damage_paths if p in test_damage_paths]\n",
    "print(\"len of overlap: \", len(overlap))\n",
    "print(\"...............\\n\")\n",
    "print(\"...............\")\n",
    "train_no_damage_paths = random.sample(no_damage_all_paths, int(len(no_damage_all_paths)*0.8))\n",
    "print(\"train no damage image count: \", len(train_no_damage_paths))\n",
    "test_no_damage_paths = [ p for p in no_damage_all_paths if p not in train_no_damage_paths]\n",
    "print(\"test no damage image count: \", len(test_no_damage_paths))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in train_no_damage_paths if p in test_no_damage_paths]\n",
    "print(\"len of overlap: \", len(overlap))\n",
    "print(\"...............\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2afad157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in train/damage:  13626\n",
      "Files in train/no_damage:  6869\n",
      "Files in test/damage:  5124\n",
      "Files in test/no_damage:  2579\n"
     ]
    }
   ],
   "source": [
    "# copying of files in the train and test directories\n",
    "import shutil\n",
    "\n",
    "root_dir = 'data_all_modified'\n",
    "split_root_dir = 'data'\n",
    "\n",
    "# Copy damaged images to train and test directories\n",
    "for p in train_damage_paths:\n",
    "    shutil.copyfile(os.path.join(root_dir, 'damage', p), os.path.join(split_root_dir, 'train/damage', p))\n",
    "\n",
    "for p in test_damage_paths:\n",
    "    shutil.copyfile(os.path.join(root_dir, 'damage', p), os.path.join(split_root_dir, 'test/damage', p))\n",
    "\n",
    "# Copy no damage images to train and test directories\n",
    "for p in train_no_damage_paths:\n",
    "    shutil.copyfile(os.path.join(root_dir, 'no_damage', p), os.path.join(split_root_dir, 'train/no_damage', p))\n",
    "\n",
    "for p in test_no_damage_paths:\n",
    "    shutil.copyfile(os.path.join(root_dir, 'no_damage', p), os.path.join(split_root_dir, 'test/no_damage', p))\n",
    "\n",
    "# Check counts to ensure files are copied correctly\n",
    "print(\"Files in train/damage: \", len(os.listdir(os.path.join(split_root_dir, \"train/damage\"))))\n",
    "print(\"Files in train/no_damage: \", len(os.listdir(os.path.join(split_root_dir, \"train/no_damage\"))))\n",
    "print(\"Files in test/damage: \", len(os.listdir(os.path.join(split_root_dir, \"test/damage\"))))\n",
    "print(\"Files in test/no_damage: \", len(os.listdir(os.path.join(split_root_dir, \"test/no_damage\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9cb1b",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aac95c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 21:58:17.326966: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-10 21:58:17.346917: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-10 21:58:17.530265: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-10 21:58:17.530356: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-10 21:58:17.557042: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-10 21:58:17.628285: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-10 21:58:17.629626: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-10 21:58:20.827231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20495 files belonging to 2 classes.\n",
      "Using 16396 files for training.\n",
      "Using 4099 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "train_data_dir = 'data/train/'\n",
    "\n",
    "batch_size = 32\n",
    "# target image size\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# note that subset=\"training\", \"validation\", \"both\", and dictates which dataset is returned\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "train_data_dir,\n",
    "validation_split=0.2,\n",
    "subset=\"both\",\n",
    "seed=123,\n",
    "image_size=(img_height, img_width),\n",
    "batch_size=batch_size\n",
    ")\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "train_rescale_ds = train_ds.map(lambda image,label:(rescale(image),label))\n",
    "val_rescale_ds = val_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac06c011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7703 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = 'data/test/'\n",
    "\n",
    "batch_size = 2\n",
    "# target image size\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# note that subset=\"training\", \"validation\", \"both\", and dictates what is returned\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "test_data_dir,\n",
    "seed=123,\n",
    "image_size=(img_height, img_width),\n",
    ")\n",
    "\n",
    "# approach 1: manually rescale data --\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "test_rescale_ds = test_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b22c86",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71695af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 49152)             0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4)                 196612    \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               640       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 197510 (771.52 KB)\n",
      "Trainable params: 197510 (771.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "model_ann = Sequential()\n",
    "\n",
    "# Flatten\n",
    "model_ann.add(Flatten(input_shape=(img_height, img_width, 3)))\n",
    "\n",
    "# Our input layer can have any number of perceptrons, we chose 2, however,\n",
    "# the input dimension must match the number of features in the independent variable -- therefore, we set\n",
    "# it to 4\n",
    "model_ann.add(Dense(120, input_dim=2, activation='relu'))\n",
    "\n",
    "# we can add any number of hidden layers with any number of perceptrons; here we choose 1 layer with 128 perceptrons. The\n",
    "# hidden layers should all use RELU\n",
    "model_ann.add(Dense(128, activation='relu'))\n",
    "\n",
    "# softmax activation function is selected for multi-label classification problems; there are 3 perceptrons in this\n",
    "# last layer because there are 2 target labels to predict (it matches the shape of y)\n",
    "model_ann.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "model_ann.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# generating summary of model\n",
    "model_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b390b1b7-44aa-4ea3-aeb5-602975773f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 2) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#fit the model from image generator\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history_ann \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_ann\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_rescale_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_rescale_ds\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_ps7m6vq.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.11/site-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 2) are incompatible\n"
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history_ann = model_ann.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1754d-f754-4ef2-80c9-b8c7480594a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_ann, test_accuracy_ann = model_ann.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823b71e-8bff-4c0e-bc66-a29be96ca2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann.save(\"models/ann.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e13dc8-ede4-4201-909d-1a7df6ffa33a",
   "metadata": {},
   "source": [
    "### LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e077cdd0-fcd9-4438-9bf7-0b11fddd3328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 126, 126, 6)       168       \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 63, 63, 6)         0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 61, 61, 16)        880       \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 30, 30, 16)        0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 14400)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 120)               1728120   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1739587 (6.64 MB)\n",
      "Trainable params: 1739587 (6.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "\n",
    "model_lenet5 = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 6 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(6, kernel_size=(3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 16 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "model_lenet5.add(layers.Flatten())\n",
    "\n",
    "# Layer 3: Fully connected layer with 120 neurons\n",
    "model_lenet5.add(layers.Dense(120, activation='relu'))\n",
    "\n",
    "# Layer 4: Fully connected layer with 84 neurons\n",
    "model_lenet5.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 3 )\n",
    "model_lenet5.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_lenet5.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_lenet5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d46f4c95-b6d4-42a7-99cf-ecafcfbeff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "513/513 [==============================] - 30s 58ms/step - loss: 0.5824 - accuracy: 0.7065 - val_loss: 0.6380 - val_accuracy: 0.6738\n",
      "Epoch 2/20\n",
      "513/513 [==============================] - 26s 51ms/step - loss: 0.4836 - accuracy: 0.7892 - val_loss: 0.6720 - val_accuracy: 0.6209\n",
      "Epoch 3/20\n",
      "513/513 [==============================] - 28s 54ms/step - loss: 0.4290 - accuracy: 0.8242 - val_loss: 0.4622 - val_accuracy: 0.7768\n",
      "Epoch 4/20\n",
      "513/513 [==============================] - 28s 54ms/step - loss: 0.3900 - accuracy: 0.8493 - val_loss: 0.3656 - val_accuracy: 0.8522\n",
      "Epoch 5/20\n",
      "513/513 [==============================] - 27s 52ms/step - loss: 0.3598 - accuracy: 0.8623 - val_loss: 0.3580 - val_accuracy: 0.8727\n",
      "Epoch 6/20\n",
      "513/513 [==============================] - 26s 50ms/step - loss: 0.3307 - accuracy: 0.8749 - val_loss: 0.3273 - val_accuracy: 0.8841\n",
      "Epoch 7/20\n",
      "513/513 [==============================] - 26s 50ms/step - loss: 0.3031 - accuracy: 0.8853 - val_loss: 0.2951 - val_accuracy: 0.8892\n",
      "Epoch 8/20\n",
      "513/513 [==============================] - 29s 56ms/step - loss: 0.2843 - accuracy: 0.8918 - val_loss: 0.3236 - val_accuracy: 0.8763\n",
      "Epoch 9/20\n",
      "513/513 [==============================] - 26s 50ms/step - loss: 0.2624 - accuracy: 0.8982 - val_loss: 0.2721 - val_accuracy: 0.8949\n",
      "Epoch 10/20\n",
      "513/513 [==============================] - 25s 49ms/step - loss: 0.2426 - accuracy: 0.9089 - val_loss: 0.4802 - val_accuracy: 0.7987\n",
      "Epoch 11/20\n",
      "513/513 [==============================] - 28s 53ms/step - loss: 0.2222 - accuracy: 0.9148 - val_loss: 0.2523 - val_accuracy: 0.9027\n",
      "Epoch 12/20\n",
      "513/513 [==============================] - 27s 52ms/step - loss: 0.2076 - accuracy: 0.9203 - val_loss: 0.3895 - val_accuracy: 0.8463\n",
      "Epoch 13/20\n",
      "513/513 [==============================] - 26s 51ms/step - loss: 0.1900 - accuracy: 0.9266 - val_loss: 0.2216 - val_accuracy: 0.9146\n",
      "Epoch 14/20\n",
      "513/513 [==============================] - 28s 54ms/step - loss: 0.1770 - accuracy: 0.9313 - val_loss: 0.2099 - val_accuracy: 0.9185\n",
      "Epoch 15/20\n",
      "513/513 [==============================] - 26s 51ms/step - loss: 0.1631 - accuracy: 0.9359 - val_loss: 0.2289 - val_accuracy: 0.9153\n",
      "Epoch 16/20\n",
      "513/513 [==============================] - 26s 51ms/step - loss: 0.1512 - accuracy: 0.9410 - val_loss: 0.3143 - val_accuracy: 0.8846\n",
      "Epoch 17/20\n",
      "513/513 [==============================] - 26s 49ms/step - loss: 0.1428 - accuracy: 0.9453 - val_loss: 0.4386 - val_accuracy: 0.8426\n",
      "Epoch 18/20\n",
      "513/513 [==============================] - 26s 50ms/step - loss: 0.1318 - accuracy: 0.9465 - val_loss: 0.3399 - val_accuracy: 0.8734\n",
      "Epoch 19/20\n",
      "513/513 [==============================] - 26s 50ms/step - loss: 0.1221 - accuracy: 0.9525 - val_loss: 0.1935 - val_accuracy: 0.9222\n",
      "Epoch 20/20\n",
      "513/513 [==============================] - 24s 47ms/step - loss: 0.1127 - accuracy: 0.9554 - val_loss: 0.1844 - val_accuracy: 0.9324\n"
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history = model_lenet5.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345e710-0833-4660-8e47-4816aafe2bc5",
   "metadata": {},
   "source": [
    "The Lenet-5 model accuracy is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a04ec671-19a0-46a5-9d78-394b237ecd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9524860382080078"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_lenet5, test_accuracy_lenet5 = model_lenet5.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy_lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be9acbde-1967-4686-859a-5d2ecdcbcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lenet5.save(\"models/lenet5.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59fea5-15c6-4906-b464-b39c2200cb61",
   "metadata": {},
   "source": [
    "### Alt LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c459c1dc-94f1-4187-b93e-5fa3d15827e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 126, 126, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 63, 63, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 30, 30, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 14, 14, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 120)               553080    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 3)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 804331 (3.07 MB)\n",
      "Trainable params: 804331 (3.07 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "\n",
    "model_altlenet = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 32 filters of size 3x3, followed by max pooling\n",
    "model_altlenet.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "model_altlenet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 64 filters of size 3x3, followed by max pooling\n",
    "model_altlenet.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model_altlenet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 3: Convolutional layer with 128 filters of size 3x3, followed by max pooling\n",
    "model_altlenet.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model_altlenet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 4: Convolutional layer with 128 filters of size 3x3, followed by max pooling\n",
    "model_altlenet.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model_altlenet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "model_altlenet.add(layers.Flatten())\n",
    "\n",
    "# Adding dropout prevents overfitting\n",
    "model_altlenet.add(layers.Dropout(0.2))\n",
    "\n",
    "# Layer 5: Fully connected layer with 120 neurons\n",
    "model_altlenet.add(layers.Dense(120, activation='relu'))\n",
    "\n",
    "# Layer 6: Fully connected layer with 84 neurons\n",
    "model_altlenet.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 3 )\n",
    "model_altlenet.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_altlenet.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_altlenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55c0a9d0-e5d9-462a-a49c-e42c90fa885f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "513/513 [==============================] - 113s 219ms/step - loss: 0.5717 - accuracy: 0.7241 - val_loss: 0.6780 - val_accuracy: 0.5975\n",
      "Epoch 2/20\n",
      "513/513 [==============================] - 111s 215ms/step - loss: 0.4009 - accuracy: 0.8289 - val_loss: 0.3459 - val_accuracy: 0.8617\n",
      "Epoch 3/20\n",
      "513/513 [==============================] - 113s 220ms/step - loss: 0.3164 - accuracy: 0.8699 - val_loss: 0.2871 - val_accuracy: 0.8827\n",
      "Epoch 4/20\n",
      "513/513 [==============================] - 113s 219ms/step - loss: 0.2422 - accuracy: 0.9025 - val_loss: 0.2473 - val_accuracy: 0.9046\n",
      "Epoch 5/20\n",
      "513/513 [==============================] - 135s 264ms/step - loss: 0.1977 - accuracy: 0.9215 - val_loss: 0.2270 - val_accuracy: 0.9127\n",
      "Epoch 6/20\n",
      "513/513 [==============================] - 156s 302ms/step - loss: 0.1649 - accuracy: 0.9351 - val_loss: 0.1657 - val_accuracy: 0.9361\n",
      "Epoch 7/20\n",
      "513/513 [==============================] - 132s 257ms/step - loss: 0.1412 - accuracy: 0.9423 - val_loss: 0.1267 - val_accuracy: 0.9490\n",
      "Epoch 8/20\n",
      "513/513 [==============================] - 136s 265ms/step - loss: 0.1245 - accuracy: 0.9507 - val_loss: 0.1194 - val_accuracy: 0.9544\n",
      "Epoch 9/20\n",
      "513/513 [==============================] - 128s 250ms/step - loss: 0.1117 - accuracy: 0.9560 - val_loss: 0.2397 - val_accuracy: 0.9188\n",
      "Epoch 10/20\n",
      "513/513 [==============================] - 129s 251ms/step - loss: 0.1058 - accuracy: 0.9581 - val_loss: 0.1451 - val_accuracy: 0.9475\n",
      "Epoch 11/20\n",
      "513/513 [==============================] - 133s 259ms/step - loss: 0.0924 - accuracy: 0.9630 - val_loss: 0.0910 - val_accuracy: 0.9654\n",
      "Epoch 12/20\n",
      "513/513 [==============================] - 124s 241ms/step - loss: 0.0872 - accuracy: 0.9652 - val_loss: 0.0878 - val_accuracy: 0.9668\n",
      "Epoch 13/20\n",
      "513/513 [==============================] - 123s 239ms/step - loss: 0.0819 - accuracy: 0.9679 - val_loss: 0.0940 - val_accuracy: 0.9663\n",
      "Epoch 14/20\n",
      "513/513 [==============================] - 125s 243ms/step - loss: 0.0775 - accuracy: 0.9707 - val_loss: 0.1417 - val_accuracy: 0.9529\n",
      "Epoch 15/20\n",
      "513/513 [==============================] - 132s 257ms/step - loss: 0.0742 - accuracy: 0.9721 - val_loss: 0.0844 - val_accuracy: 0.9712\n",
      "Epoch 16/20\n",
      "513/513 [==============================] - 133s 259ms/step - loss: 0.0669 - accuracy: 0.9738 - val_loss: 0.3284 - val_accuracy: 0.8724\n",
      "Epoch 17/20\n",
      "513/513 [==============================] - 135s 263ms/step - loss: 0.0631 - accuracy: 0.9753 - val_loss: 0.1790 - val_accuracy: 0.9346\n",
      "Epoch 18/20\n",
      "513/513 [==============================] - 114s 222ms/step - loss: 0.0610 - accuracy: 0.9781 - val_loss: 0.0773 - val_accuracy: 0.9746\n",
      "Epoch 19/20\n",
      "513/513 [==============================] - 132s 258ms/step - loss: 0.0568 - accuracy: 0.9780 - val_loss: 0.0750 - val_accuracy: 0.9715\n",
      "Epoch 20/20\n",
      "513/513 [==============================] - 132s 257ms/step - loss: 0.0549 - accuracy: 0.9787 - val_loss: 0.0779 - val_accuracy: 0.9717\n"
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history_altlenet = model_altlenet.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0702bbfb-e618-481c-a6e6-9c4ae5071046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787095785140991"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_altlenet, test_accuracy_altlenet = model_altlenet.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy_altlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "479f2e50-5d81-4e73-b2aa-3db1ea43d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lenet5.save(\"models/altlenet.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad953404-bf57-4749-af8c-46a80ebafc1c",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f941095-0c6f-40a3-9bfc-5413a4ffe8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# grab an entry from X_test -- here, we grab the first one\n",
    "l = X_test[0].tolist()\n",
    "\n",
    "# make the POST request passing the single test case as the `image` field:\n",
    "rsp = requests.post(\"http://172.17.0.1:5000/models/\", json={\"image\": l})\n",
    "\n",
    "# print the json response\n",
    "rsp.json()\n",
    "\n",
    "{'result': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
