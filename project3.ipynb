{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f675411",
   "metadata": {},
   "source": [
    "# Project 2 Model Classification\n",
    "### Serena Shah, Osvaldo Salinas\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966bfb7",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87799c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"data/train/damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/train/no_damage\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(\"data/test/damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/test/no_damage\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93b108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need paths of images for individual classes so we can copy them in the new directories that we created above\n",
    "\n",
    "damage_all_paths = os.listdir('data_all_modified/damage')\n",
    "no_damage_all_paths = os.listdir('data_all_modified/no_damage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea54e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      "train damage image count:  11336\n",
      "test damage image count:  2834\n",
      "len of overlap:  0\n",
      "...............\n",
      "\n",
      "...............\n",
      "train no damage image count:  5721\n",
      "test no damage image count:  1431\n",
      "len of overlap:  0\n",
      "...............\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the image paths into train and test by randomly selecting 80% of the images in train and 20% in test.\n",
    "import random\n",
    "\n",
    "print(\"...............\")\n",
    "train_damage_paths = random.sample(damage_all_paths, int(len(damage_all_paths)*0.8))\n",
    "print(\"train damage image count: \", len(train_damage_paths))\n",
    "test_damage_paths = [ p for p in damage_all_paths if p not in train_damage_paths]\n",
    "print(\"test damage image count: \", len(test_damage_paths))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in train_damage_paths if p in test_damage_paths]\n",
    "print(\"len of overlap: \", len(overlap))\n",
    "print(\"...............\\n\")\n",
    "print(\"...............\")\n",
    "train_no_damage_paths = random.sample(no_damage_all_paths, int(len(no_damage_all_paths)*0.8))\n",
    "print(\"train no damage image count: \", len(train_no_damage_paths))\n",
    "test_no_damage_paths = [ p for p in no_damage_all_paths if p not in train_no_damage_paths]\n",
    "print(\"test no damage image count: \", len(test_no_damage_paths))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in train_no_damage_paths if p in test_no_damage_paths]\n",
    "print(\"len of overlap: \", len(overlap))\n",
    "print(\"...............\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2afad157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in train/damage:  14170\n",
      "Files in train/no_damage:  7152\n",
      "Files in test/damage:  11186\n",
      "Files in test/no_damage:  5617\n"
     ]
    }
   ],
   "source": [
    "# copying of files in the train and test directories\n",
    "import shutil\n",
    "\n",
    "root_dir = 'data_all_modified'\n",
    "split_root_dir = 'data'\n",
    "\n",
    "# Copy damaged images to train and test directories\n",
    "for p in train_damage_paths:\n",
    "    shutil.copyfile(os.path.join(root_dir, 'damage', p), os.path.join(split_root_dir, 'train/damage', p))\n",
    "\n",
    "for p in test_damage_paths:\n",
    "    shutil.copyfile(os.path.join(root_dir, 'damage', p), os.path.join(split_root_dir, 'test/damage', p))\n",
    "\n",
    "# Copy no damage images to train and test directories\n",
    "for p in train_no_damage_paths:\n",
    "    shutil.copyfile(os.path.join(root_dir, 'no_damage', p), os.path.join(split_root_dir, 'train/no_damage', p))\n",
    "\n",
    "for p in test_no_damage_paths:\n",
    "    shutil.copyfile(os.path.join(root_dir, 'no_damage', p), os.path.join(split_root_dir, 'test/no_damage', p))\n",
    "\n",
    "# Check counts to ensure files are copied correctly\n",
    "print(\"Files in train/damage: \", len(os.listdir(os.path.join(split_root_dir, \"train/damage\"))))\n",
    "print(\"Files in train/no_damage: \", len(os.listdir(os.path.join(split_root_dir, \"train/no_damage\"))))\n",
    "print(\"Files in test/damage: \", len(os.listdir(os.path.join(split_root_dir, \"test/damage\"))))\n",
    "print(\"Files in test/no_damage: \", len(os.listdir(os.path.join(split_root_dir, \"test/no_damage\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9cb1b",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aac95c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: protobuf\n",
      "Version: 4.23.4\n",
      "Summary: \n",
      "Home-page: https://developers.google.com/protocol-buffers/\n",
      "Author: protobuf@googlegroups.com\n",
      "Author-email: protobuf@googlegroups.com\n",
      "License: 3-Clause BSD License\n",
      "Location: /usr/local/lib/python3.11/site-packages\n",
      "Requires: \n",
      "Required-by: tensorboard, tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 19:34:02.755844: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-10 19:34:02.833571: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-10 19:34:03.313368: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-10 19:34:03.313614: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-10 19:34:03.415494: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-10 19:34:03.621048: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-10 19:34:03.627468: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-10 19:34:09.067276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21322 files belonging to 2 classes.\n",
      "Using 17058 files for training.\n",
      "Using 4264 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "train_data_dir = 'data/train/'\n",
    "\n",
    "batch_size = 32\n",
    "# target image size\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# note that subset=\"training\", \"validation\", \"both\", and dictates which dataset is returned\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "train_data_dir,\n",
    "validation_split=0.2,\n",
    "subset=\"both\",\n",
    "seed=123,\n",
    "image_size=(img_height, img_width),\n",
    "batch_size=batch_size\n",
    ")\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "train_rescale_ds = train_ds.map(lambda image,label:(rescale(image),label))\n",
    "val_rescale_ds = val_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac06c011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16803 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = 'data/test/'\n",
    "\n",
    "batch_size = 2\n",
    "# target image size\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# note that subset=\"training\", \"validation\", \"both\", and dictates what is returned\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "test_data_dir,\n",
    "seed=123,\n",
    "image_size=(img_height, img_width),\n",
    ")\n",
    "\n",
    "# approach 1: manually rescale data --\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "test_rescale_ds = test_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b22c86",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71695af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 128, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 64, 64, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 32)        18464     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 32, 32, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               819300    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                8484      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 857458 (3.27 MB)\n",
      "Trainable params: 857458 (3.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "print(os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'])\n",
    "\n",
    "import keras \n",
    "from keras import layers\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "\n",
    "# Intializing a sequential model\n",
    "model_cnn = models.Sequential()\n",
    "\n",
    "# Adding first conv layer with 64 filters and kernel size 3x3 , padding 'same' provides the output size same as the input size\n",
    "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=(img_height, img_width, 3)))\n",
    "\n",
    "# Adding max pooling to reduce the size of output of first conv layer\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2), padding = 'same'))\n",
    "\n",
    "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2), padding = 'same'))\n",
    "\n",
    "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2), padding = 'same'))\n",
    "\n",
    "# flattening the output of the conv layer after max pooling to make it ready for creating dense connections\n",
    "model_cnn.add(layers.Flatten())\n",
    "\n",
    "# Adding a fully connected dense layer with 100 neurons\n",
    "model_cnn.add(layers.Dense(100, activation='relu'))\n",
    "\n",
    "# Adding a fully connected dense layer with 84 neurons\n",
    "model_cnn.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Adding the output layer with * neurons and activation functions as softmax since this is a multi-class classification problem\n",
    "model_cnn.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "# RMSprop (Root Mean Square Propagation) is commonly used in training deep neural networks.\n",
    "model_cnn.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e021d3d-7781-4ced-ae60-3fc39f312870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "534/534 [==============================] - 133s 247ms/step - loss: 0.5191 - accuracy: 0.7558 - val_loss: 0.5933 - val_accuracy: 0.7022\n",
      "Epoch 2/20\n",
      "534/534 [==============================] - 134s 251ms/step - loss: 0.3855 - accuracy: 0.8385 - val_loss: 0.4098 - val_accuracy: 0.8159\n",
      "Epoch 3/20\n",
      "534/534 [==============================] - 124s 232ms/step - loss: 0.2969 - accuracy: 0.8833 - val_loss: 0.3974 - val_accuracy: 0.8276\n",
      "Epoch 4/20\n",
      "534/534 [==============================] - 122s 228ms/step - loss: 0.2248 - accuracy: 0.9144 - val_loss: 0.2636 - val_accuracy: 0.8926\n",
      "Epoch 5/20\n",
      "534/534 [==============================] - 129s 242ms/step - loss: 0.1843 - accuracy: 0.9285 - val_loss: 0.1674 - val_accuracy: 0.9350\n",
      "Epoch 6/20\n",
      "534/534 [==============================] - 146s 273ms/step - loss: 0.1579 - accuracy: 0.9392 - val_loss: 0.1508 - val_accuracy: 0.9425\n",
      "Epoch 7/20\n",
      "534/534 [==============================] - 152s 284ms/step - loss: 0.1404 - accuracy: 0.9452 - val_loss: 0.1307 - val_accuracy: 0.9454\n",
      "Epoch 8/20\n",
      "534/534 [==============================] - 142s 266ms/step - loss: 0.1243 - accuracy: 0.9518 - val_loss: 0.2116 - val_accuracy: 0.9163\n",
      "Epoch 9/20\n",
      "534/534 [==============================] - 139s 260ms/step - loss: 0.1166 - accuracy: 0.9540 - val_loss: 0.1658 - val_accuracy: 0.9383\n",
      "Epoch 10/20\n",
      "534/534 [==============================] - 123s 230ms/step - loss: 0.1059 - accuracy: 0.9586 - val_loss: 0.1005 - val_accuracy: 0.9594\n",
      "Epoch 11/20\n",
      "534/534 [==============================] - 142s 265ms/step - loss: 0.0968 - accuracy: 0.9622 - val_loss: 0.0997 - val_accuracy: 0.9615\n",
      "Epoch 12/20\n",
      "534/534 [==============================] - 126s 236ms/step - loss: 0.0907 - accuracy: 0.9669 - val_loss: 0.0939 - val_accuracy: 0.9604\n",
      "Epoch 13/20\n",
      "534/534 [==============================] - 135s 251ms/step - loss: 0.0829 - accuracy: 0.9678 - val_loss: 0.0899 - val_accuracy: 0.9636\n",
      "Epoch 14/20\n",
      "534/534 [==============================] - 118s 221ms/step - loss: 0.0771 - accuracy: 0.9691 - val_loss: 0.0901 - val_accuracy: 0.9655\n",
      "Epoch 15/20\n",
      "534/534 [==============================] - 111s 207ms/step - loss: 0.0707 - accuracy: 0.9730 - val_loss: 0.1076 - val_accuracy: 0.9597\n",
      "Epoch 16/20\n",
      "534/534 [==============================] - 113s 212ms/step - loss: 0.0665 - accuracy: 0.9762 - val_loss: 1.0456 - val_accuracy: 0.6567\n",
      "Epoch 17/20\n",
      "534/534 [==============================] - 112s 210ms/step - loss: 0.0643 - accuracy: 0.9771 - val_loss: 0.0987 - val_accuracy: 0.9636\n",
      "Epoch 18/20\n",
      " 73/534 [===>..........................] - ETA: 1:33 - loss: 0.0573 - accuracy: 0.9752"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#fit the model from image generator\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history_ann \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_rescale_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_rescale_ds\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history_ann = model_cnn.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1754d-f754-4ef2-80c9-b8c7480594a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_ann, test_accuracy_ann = model_cnn.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e13dc8-ede4-4201-909d-1a7df6ffa33a",
   "metadata": {},
   "source": [
    "### LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e077cdd0-fcd9-4438-9bf7-0b11fddd3328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 126, 126, 6)       168       \n",
      "                                                                 \n",
      " average_pooling2d_6 (Avera  (None, 63, 63, 6)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 61, 61, 16)        880       \n",
      "                                                                 \n",
      " average_pooling2d_7 (Avera  (None, 30, 30, 16)        0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 14400)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 120)               1728120   \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 3)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1739587 (6.64 MB)\n",
      "Trainable params: 1739587 (6.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "\n",
    "model_lenet5 = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 6 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(6, kernel_size=(3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 16 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "model_lenet5.add(layers.Flatten())\n",
    "\n",
    "# Layer 3: Fully connected layer with 120 neurons\n",
    "model_lenet5.add(layers.Dense(120, activation='relu'))\n",
    "\n",
    "# Layer 4: Fully connected layer with 84 neurons\n",
    "model_lenet5.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 3 )\n",
    "model_lenet5.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_lenet5.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_lenet5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d46f4c95-b6d4-42a7-99cf-ecafcfbeff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "534/534 [==============================] - 43s 78ms/step - loss: 0.5916 - accuracy: 0.7025 - val_loss: 0.5579 - val_accuracy: 0.7324\n",
      "Epoch 2/20\n",
      "534/534 [==============================] - 33s 61ms/step - loss: 0.4663 - accuracy: 0.8068 - val_loss: 0.6121 - val_accuracy: 0.7038\n",
      "Epoch 3/20\n",
      "534/534 [==============================] - 32s 59ms/step - loss: 0.4154 - accuracy: 0.8343 - val_loss: 0.5275 - val_accuracy: 0.7549\n",
      "Epoch 4/20\n",
      "534/534 [==============================] - 32s 60ms/step - loss: 0.3883 - accuracy: 0.8460 - val_loss: 0.5356 - val_accuracy: 0.7655\n",
      "Epoch 5/20\n",
      "534/534 [==============================] - 32s 59ms/step - loss: 0.3737 - accuracy: 0.8546 - val_loss: 0.4538 - val_accuracy: 0.7976\n",
      "Epoch 6/20\n",
      "534/534 [==============================] - 33s 61ms/step - loss: 0.3519 - accuracy: 0.8637 - val_loss: 0.3352 - val_accuracy: 0.8727\n",
      "Epoch 7/20\n",
      "534/534 [==============================] - 31s 57ms/step - loss: 0.3326 - accuracy: 0.8717 - val_loss: 0.3992 - val_accuracy: 0.8483\n",
      "Epoch 8/20\n",
      "534/534 [==============================] - 33s 62ms/step - loss: 0.3169 - accuracy: 0.8771 - val_loss: 0.3060 - val_accuracy: 0.8846\n",
      "Epoch 9/20\n",
      "534/534 [==============================] - 33s 61ms/step - loss: 0.3004 - accuracy: 0.8850 - val_loss: 0.2917 - val_accuracy: 0.8891\n",
      "Epoch 10/20\n",
      "534/534 [==============================] - 34s 63ms/step - loss: 0.2856 - accuracy: 0.8920 - val_loss: 0.3883 - val_accuracy: 0.8267\n",
      "Epoch 11/20\n",
      "534/534 [==============================] - 25s 47ms/step - loss: 0.2684 - accuracy: 0.8981 - val_loss: 0.2671 - val_accuracy: 0.8994\n",
      "Epoch 12/20\n",
      "534/534 [==============================] - 27s 51ms/step - loss: 0.2532 - accuracy: 0.9005 - val_loss: 0.3214 - val_accuracy: 0.8729\n",
      "Epoch 13/20\n",
      "534/534 [==============================] - 26s 48ms/step - loss: 0.2376 - accuracy: 0.9080 - val_loss: 0.2922 - val_accuracy: 0.8841\n",
      "Epoch 14/20\n",
      "534/534 [==============================] - 28s 51ms/step - loss: 0.2213 - accuracy: 0.9156 - val_loss: 0.2362 - val_accuracy: 0.9099\n",
      "Epoch 15/20\n",
      "534/534 [==============================] - 25s 47ms/step - loss: 0.2065 - accuracy: 0.9199 - val_loss: 0.5204 - val_accuracy: 0.7282\n",
      "Epoch 16/20\n",
      "534/534 [==============================] - 33s 61ms/step - loss: 0.1931 - accuracy: 0.9244 - val_loss: 0.2198 - val_accuracy: 0.9158\n",
      "Epoch 17/20\n",
      "534/534 [==============================] - 34s 64ms/step - loss: 0.1788 - accuracy: 0.9314 - val_loss: 0.2294 - val_accuracy: 0.9128\n",
      "Epoch 18/20\n",
      "534/534 [==============================] - 31s 58ms/step - loss: 0.1667 - accuracy: 0.9353 - val_loss: 0.2205 - val_accuracy: 0.9163\n",
      "Epoch 19/20\n",
      "534/534 [==============================] - 27s 50ms/step - loss: 0.1559 - accuracy: 0.9410 - val_loss: 0.1989 - val_accuracy: 0.9259\n",
      "Epoch 20/20\n",
      "534/534 [==============================] - 28s 53ms/step - loss: 0.1451 - accuracy: 0.9433 - val_loss: 0.1970 - val_accuracy: 0.9250\n"
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history = model_lenet5.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345e710-0833-4660-8e47-4816aafe2bc5",
   "metadata": {},
   "source": [
    "The Lenet-5 model accuracy is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a04ec671-19a0-46a5-9d78-394b237ecd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9483425617218018"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_lenet5, test_accuracy_lenet5 = model_lenet5.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy_lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be9acbde-1967-4686-859a-5d2ecdcbcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lenet5.save(\"lenet5.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59fea5-15c6-4906-b464-b39c2200cb61",
   "metadata": {},
   "source": [
    "### Alt LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c459c1dc-94f1-4187-b93e-5fa3d15827e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_11 (Conv2D)          (None, 126, 126, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 63, 63, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 30, 30, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 14, 14, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 120)               553080    \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 3)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 804331 (3.07 MB)\n",
      "Trainable params: 804331 (3.07 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "\n",
    "model_altlenet = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 32 filters of size 3x3, followed by max pooling\n",
    "model_altlenet.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "model_altlenet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 64 filters of size 3x3, followed by max pooling\n",
    "model_altlenet.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model_altlenet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 3: Convolutional layer with 128 filters of size 3x3, followed by max pooling\n",
    "model_altlenet.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model_altlenet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 4: Convolutional layer with 128 filters of size 3x3, followed by max pooling\n",
    "model_altlenet.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model_altlenet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "model_altlenet.add(layers.Flatten())\n",
    "\n",
    "# Adding dropout prevents overfitting\n",
    "model_altlenet.add(layers.Dropout(0.2))\n",
    "\n",
    "# Layer 5: Fully connected layer with 120 neurons\n",
    "model_altlenet.add(layers.Dense(120, activation='relu'))\n",
    "\n",
    "# Layer 6: Fully connected layer with 84 neurons\n",
    "model_altlenet.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 3 )\n",
    "model_altlenet.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_altlenet.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_altlenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55c0a9d0-e5d9-462a-a49c-e42c90fa885f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "534/534 [==============================] - 30s 56ms/step - loss: 0.1337 - accuracy: 0.9484 - val_loss: 0.1975 - val_accuracy: 0.9282\n",
      "Epoch 2/20\n",
      "534/534 [==============================] - 34s 64ms/step - loss: 0.1274 - accuracy: 0.9522 - val_loss: 0.2211 - val_accuracy: 0.9224\n",
      "Epoch 3/20\n",
      "534/534 [==============================] - 30s 56ms/step - loss: 0.1198 - accuracy: 0.9547 - val_loss: 0.1819 - val_accuracy: 0.9339\n",
      "Epoch 4/20\n",
      "534/534 [==============================] - 34s 62ms/step - loss: 0.1123 - accuracy: 0.9583 - val_loss: 0.2089 - val_accuracy: 0.9287\n",
      "Epoch 5/20\n",
      "534/534 [==============================] - 29s 54ms/step - loss: 0.1052 - accuracy: 0.9607 - val_loss: 0.2235 - val_accuracy: 0.9280\n",
      "Epoch 6/20\n",
      "534/534 [==============================] - 31s 57ms/step - loss: 0.0986 - accuracy: 0.9632 - val_loss: 0.1791 - val_accuracy: 0.9348\n",
      "Epoch 7/20\n",
      "534/534 [==============================] - 23s 43ms/step - loss: 0.0897 - accuracy: 0.9672 - val_loss: 0.1789 - val_accuracy: 0.9381\n",
      "Epoch 8/20\n",
      "534/534 [==============================] - 22s 40ms/step - loss: 0.0863 - accuracy: 0.9701 - val_loss: 0.2000 - val_accuracy: 0.9329\n",
      "Epoch 9/20\n",
      "534/534 [==============================] - 22s 40ms/step - loss: 0.0800 - accuracy: 0.9714 - val_loss: 0.2095 - val_accuracy: 0.9296\n",
      "Epoch 10/20\n",
      "534/534 [==============================] - 25s 47ms/step - loss: 0.0744 - accuracy: 0.9734 - val_loss: 0.1870 - val_accuracy: 0.9355\n",
      "Epoch 11/20\n",
      "534/534 [==============================] - 24s 44ms/step - loss: 0.0689 - accuracy: 0.9766 - val_loss: 0.2203 - val_accuracy: 0.9299\n",
      "Epoch 12/20\n",
      "534/534 [==============================] - 28s 52ms/step - loss: 0.0666 - accuracy: 0.9768 - val_loss: 0.2364 - val_accuracy: 0.9313\n",
      "Epoch 13/20\n",
      "534/534 [==============================] - 31s 58ms/step - loss: 0.0584 - accuracy: 0.9799 - val_loss: 0.2221 - val_accuracy: 0.9308\n",
      "Epoch 14/20\n",
      "534/534 [==============================] - 39s 67ms/step - loss: 0.0547 - accuracy: 0.9822 - val_loss: 0.4148 - val_accuracy: 0.8938\n",
      "Epoch 15/20\n",
      "534/534 [==============================] - 33s 61ms/step - loss: 0.0490 - accuracy: 0.9844 - val_loss: 0.2269 - val_accuracy: 0.9332\n",
      "Epoch 16/20\n",
      "534/534 [==============================] - 28s 51ms/step - loss: 0.0464 - accuracy: 0.9851 - val_loss: 0.2556 - val_accuracy: 0.9240\n",
      "Epoch 17/20\n",
      "534/534 [==============================] - 22s 40ms/step - loss: 0.0440 - accuracy: 0.9856 - val_loss: 0.3048 - val_accuracy: 0.9081\n",
      "Epoch 18/20\n",
      "534/534 [==============================] - 22s 40ms/step - loss: 0.0408 - accuracy: 0.9865 - val_loss: 0.2250 - val_accuracy: 0.9348\n",
      "Epoch 19/20\n",
      "534/534 [==============================] - 21s 40ms/step - loss: 0.0379 - accuracy: 0.9870 - val_loss: 0.3064 - val_accuracy: 0.9233\n",
      "Epoch 20/20\n",
      "534/534 [==============================] - 22s 40ms/step - loss: 0.0351 - accuracy: 0.9898 - val_loss: 0.2374 - val_accuracy: 0.9329\n"
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history_altlenet = model_lenet5.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0702bbfb-e618-481c-a6e6-9c4ae5071046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 21:12:46.852030: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-93.978029_29.902963.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.875985: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-93.988838_29.890141999999997.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.876428: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-95.10424300000001_29.844515.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.877423: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-93.973794_29.899791999999998.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.886323: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-95.120418_29.978606.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.886523: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-95.066575_29.831365.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.894200: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-93.666628_30.614196999999997.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.895653: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-93.667798_30.217875.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.898370: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-95.07764499999999_29.826444.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.903651: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-94.000726_29.892937.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.912102: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-93.977326_29.904279.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.918277: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-93.674203_30.210426000000002.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.918436: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-93.943831_30.165017.jpeg; No such file or directory\n",
      "2024-04-10 21:12:46.929669: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: data/test/damage/-95.121401_29.98725.jpeg; No such file or directory\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\ndata/test/damage/-93.978029_29.902963.jpeg; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]] [Op:__inference_test_function_160824]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_loss_altlenet, test_accuracy_altlenet \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_altlenet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_rescale_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_accuracy_altlenet\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\ndata/test/damage/-93.978029_29.902963.jpeg; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]] [Op:__inference_test_function_160824]"
     ]
    }
   ],
   "source": [
    "test_loss_altlenet, test_accuracy_altlenet = model_altlenet.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy_altlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f2e50-5d81-4e73-b2aa-3db1ea43d0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
